{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAP8qCNs7n4k",
        "outputId": "a7b7f82e-4f93-4ee3-b0ea-0e4db85a0fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000\n",
            "License(s): unknown\n",
            "Downloading imagenetmini-1000.zip to /content\n",
            " 99% 3.90G/3.92G [01:02<00:00, 196MB/s]\n",
            "100% 3.92G/3.92G [01:03<00:00, 66.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle torch torchvision tqdm matplotlib\n",
        "\n",
        "# --- Kaggle setup (upload kaggle.json first) ---\n",
        "!mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d ifigotin/imagenetmini-1000 -p /content\n",
        "!unzip -q /content/imagenetmini-1000.zip -d /content/imagenet-mini\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "NUM_CLASSES = 1000\n",
        "BATCH_SIZE  = 256          # larger batch speeds convergence\n",
        "EPOCHS      = 30\n",
        "MAX_LR      = 0.2          # higher LR works with OneCycle\n",
        "MOMENTUM    = 0.9\n",
        "WEIGHT_DECAY = 1e-4\n",
        "LABEL_SMOOTH = 0.1\n",
        "\n",
        "mean = [0.485,0.456,0.406]; std=[0.229,0.224,0.225]\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std)\n",
        "])\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std)\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(\"/content/imagenet-mini/imagenet-mini/train\", transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(\"/content/imagenet-mini/imagenet-mini/val\",   transform=val_tfms)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0lVF8-J8HgA",
        "outputId": "e01f4091-7c26-4beb-be18-85580e34d129"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_bbox(size, lam):\n",
        "    W,H = size[2], size[3]\n",
        "    cut_rat = (1 - lam) ** 0.5\n",
        "    cut_w, cut_h = int(W*cut_rat), int(H*cut_rat)\n",
        "    cx, cy = random.randint(0, W), random.randint(0, H)\n",
        "    x1, y1 = max(cx-cut_w//2,0), max(cy-cut_h//2,0)\n",
        "    x2, y2 = min(cx+cut_w//2,W), min(cy+cut_h//2,H)\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def mixup_cutmix_data(x, y, alpha=1.0, mixup_prob=0.5):\n",
        "    if random.random() < mixup_prob:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        batch_size = x.size()[0]\n",
        "        index = torch.randperm(batch_size).to(x.device)\n",
        "        if random.random() < 0.5:     # MixUp\n",
        "            mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "        else:                         # CutMix\n",
        "            x1,y1,x2,y2 = rand_bbox(x.size(), lam)\n",
        "            mixed_x = x.clone()\n",
        "            mixed_x[:,:,x1:x2,y1:y2] = x[index,:,x1:x2,y1:y2]\n",
        "            lam = 1 - ((x2-x1)*(y2-y1)/(x.size()[-1]*x.size()[-2]))\n",
        "        y_a, y_b = y, y[index]\n",
        "        return mixed_x, y_a, y_b, lam\n",
        "    else:\n",
        "        return x, y, y, 1.0\n"
      ],
      "metadata": {
        "id": "JwG3rpIU8KiV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50(weights=None, num_classes=NUM_CLASSES).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "optimizer = optim.SGD(model.parameters(), lr=MAX_LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# OneCycleLR setup\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=MAX_LR,\n",
        "    steps_per_epoch=len(train_loader), epochs=EPOCHS,\n",
        "    pct_start=0.3, anneal_strategy='cos', div_factor=25.0, final_div_factor=1e4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38PPvAjh8Pza",
        "outputId": "b89a04e1-e2e1-4c2a-afd1-58ec7fd29310"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2125695137.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy, numpy as np\n",
        "def lr_finder(model, loader, optimizer, init_value=1e-5, final_value=1, beta=0.98):\n",
        "    num = len(loader)-1\n",
        "    mult = (final_value / init_value) ** (1/num)\n",
        "    lr = init_value; optimizer.param_groups[0]['lr']=lr\n",
        "    avg_loss, best_loss = 0.,0.; losses=[]; log_lrs=[]\n",
        "    for i,(x,y) in enumerate(loader):\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(x); loss = criterion(out,y)\n",
        "        avg_loss = beta*avg_loss + (1-beta)*loss.item()\n",
        "        smoothed = avg_loss/(1-beta**(i+1))\n",
        "        if i>0 and smoothed>4*best_loss: break\n",
        "        if smoothed<best_loss or i==0: best_loss = smoothed\n",
        "        losses.append(smoothed); log_lrs.append(np.log10(lr))\n",
        "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
        "        lr *= mult; optimizer.param_groups[0]['lr']=lr\n",
        "    plt.plot(log_lrs,losses); plt.xlabel(\"log10 LR\"); plt.ylabel(\"Loss\"); plt.title(\"LR Finder\"); plt.show()\n",
        "\n",
        "# optional quick check\n",
        "# lr_finder(copy.deepcopy(model), train_loader, optimizer)\n"
      ],
      "metadata": {
        "id": "IJitj0Rl8SKI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def accuracy_top1(model, loader):\n",
        "    model.eval(); correct=0; total=0\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        _,pred = out.max(1)\n",
        "        correct += (pred==y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return correct/total\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train(); run_loss=0; correct=0; total=0\n",
        "    for x,y in tqdm(train_loader, leave=False):\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        x,y_a,y_b,lam = mixup_cutmix_data(x,y)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(x)\n",
        "            loss = lam*criterion(out,y_a) + (1-lam)*criterion(out,y_b)\n",
        "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update(); scheduler.step()\n",
        "        run_loss += loss.item()\n",
        "        _,pred = out.max(1)\n",
        "        correct += (pred==y).sum().item(); total += y.size(0)\n",
        "    train_acc = correct/total\n",
        "    return run_loss/len(train_loader), train_acc\n",
        "\n",
        "# ---- main loop ----\n",
        "best=0; train_losses=[]; train_accs=[]; val_accs=[]\n",
        "for ep in range(EPOCHS):\n",
        "    tl,ta = train_one_epoch(ep)\n",
        "    va = accuracy_top1(model,val_loader)\n",
        "    train_losses.append(tl); train_accs.append(ta); val_accs.append(va)\n",
        "    print(f\"Epoch {ep+1:02d} | Loss {tl:.4f} | Train@1 {ta*100:.2f}% | Val@1 {va*100:.2f}%\")\n",
        "    if va>best:\n",
        "        best=va; torch.save(model.state_dict(),\"resnet50_imagenetmini_mixcut_best.pth\")\n",
        "        print(f\"✅ Saved new best model ({best*100:.2f}%)\")\n"
      ],
      "metadata": {
        "id": "SFfhIhXF8VOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training summary\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(train_losses); plt.title(\"Train Loss\")\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot([v*100 for v in train_accs],label='Train@1')\n",
        "plt.plot([v*100 for v in val_accs],label='Val@1'); plt.legend(); plt.title(\"Accuracy (%)\")\n",
        "plt.subplot(1,3,3)\n",
        "lrs = [group['lr'] for _ in range(EPOCHS*len(train_loader)) for group in optimizer.param_groups]\n",
        "plt.plot(lrs[:len(train_loader)], color='orange'); plt.title(\"One-Cycle LR (first epoch)\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "print(f\"🏁 Best Val Top-1: {best*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "puvWervq8WQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}